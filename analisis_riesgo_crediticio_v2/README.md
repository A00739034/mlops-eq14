Analisis Riesgo Crediticio V2
==============================

Analisis de riesgo crediticio del equipo 14 de la materia de MLOPS Fase 2

## üöÄ Inicio R√°pido

### Instalaci√≥n

```bash
# Instalar dependencias
pip install -r requirements.txt

# Configurar DVC con S3
python setup_dvc_s3.py

# Descargar datos desde S3 (si ya est√°n versionados)
dvc pull
```

### Versionado de Datos con DVC

Este proyecto utiliza **DVC (Data Version Control)** para versionar datos grandes y modelos, almacen√°ndolos en **Amazon S3**.

- **Documentaci√≥n completa**: Ver [DVC_S3_SETUP.md](DVC_S3_SETUP.md)
- **Configuraci√≥n r√°pida**: Ejecutar `python setup_dvc_s3.py`

**Comandos principales:**
- `dvc add <archivo>` - Agregar archivo a DVC
- `dvc push` - Subir datos a S3
- `dvc pull` - Descargar datos desde S3
- `dvc status` - Ver estado de los archivos

## ü§ñ Pipeline Automatizado de Scikit-learn

Este proyecto incluye un **pipeline completo de Scikit-learn** que automatiza todo el flujo de machine learning, desde el preprocesamiento hasta el modelo final.

### Caracter√≠sticas del Pipeline

El pipeline automatizado integra:

1. **Preprocesamiento de datos**
   - Limpieza y validaci√≥n de datos
   - Validaci√≥n de dominios categ√≥ricos y rangos continuos
   - Imputaci√≥n de valores faltantes (mediana para continuas, moda para categ√≥ricas)

2. **Ingenier√≠a de caracter√≠sticas**
   - Creaci√≥n de caracter√≠sticas de interacci√≥n
   - Creaci√≥n de caracter√≠sticas de ratio
   - Binning de variables continuas
   - Codificaci√≥n de variables categ√≥ricas (Label Encoding y One-Hot Encoding)

3. **Selecci√≥n de caracter√≠sticas**
   - Selecci√≥n basada en informaci√≥n mutua o test F
   - Reducci√≥n de dimensionalidad opcional con PCA

4. **Modelo de Machine Learning**
   - Soporte para m√∫ltiples algoritmos (Logistic Regression, Random Forest, Gradient Boosting, SVM)

### Uso R√°pido

```bash
# Entrenar con Logistic Regression
python run_sklearn_pipeline.py data/raw/german_credit_modified.csv --model logistic

# Entrenar con Random Forest y usar MLflow
python run_sklearn_pipeline.py data/raw/german_credit_modified.csv \
    --model random_forest --use-mlflow

# Configuraci√≥n personalizada
python run_sklearn_pipeline.py data/raw/german_credit_modified.csv \
    --model gradient_boosting \
    --n-features 20 \
    --no-interactions \
    --output-model models/my_model.joblib
```

### Opciones Disponibles

- `--model`: Tipo de modelo (`logistic`, `random_forest`, `gradient_boosting`, `svm`)
- `--n-features`: N√∫mero de caracter√≠sticas a seleccionar (default: 15)
- `--feature-selection-method`: M√©todo de selecci√≥n (`mutual_info`, `f_classif`)
- `--test-size`: Tama√±o del conjunto de prueba (default: 0.25)
- `--use-mlflow`: Activar tracking con MLflow
- `--no-interactions`: Desactivar caracter√≠sticas de interacci√≥n
- `--no-ratios`: Desactivar caracter√≠sticas de ratio
- `--no-binning`: Desactivar caracter√≠sticas de binning

### Uso Program√°tico

```python
from src.models.sklearn_pipeline import create_sklearn_pipeline, SklearnPipelineManager
from sklearn.ensemble import RandomForestClassifier
import pandas as pd

# Cargar datos
df = pd.read_csv("data/raw/german_credit_modified.csv")
X = df.drop(columns=["target_bad"])
y = df["target_bad"]

# Crear pipeline
pipeline = create_sklearn_pipeline(
    model=RandomForestClassifier(n_estimators=300, random_state=42),
    continuous_vars=["hoehe", "laufzeit", "alter"],
    categorical_vars=["laufkont", "moral", "verw", ...],
    scale_features=True,
    feature_selection=True,
    n_features_select=15
)

# Entrenar y evaluar
manager = SklearnPipelineManager(pipeline=pipeline)
results = manager.train_and_evaluate(X, y, use_mlflow=True)

# Guardar modelo
manager.save("models/my_pipeline.joblib")

# Cargar y usar para predicciones
manager.load("models/my_pipeline.joblib")
predictions = manager.predict(X_new)
```

### Ventajas del Pipeline Automatizado

‚úÖ **Todo en un solo objeto**: Preprocesamiento, transformaciones y modelo unificados  
‚úÖ **F√°cil de usar**: Una sola llamada `fit()` y `predict()`  
‚úÖ **Reproducible**: Todos los pasos est√°n versionados y guardados  
‚úÖ **MLflow compatible**: Tracking autom√°tico de experimentos  
‚úÖ **Listo para producci√≥n**: F√°cil de desplegar y usar en servicios

### Archivos del Pipeline

- `src/models/sklearn_pipeline.py`: Implementaci√≥n del pipeline automatizado
- `run_sklearn_pipeline.py`: Script para ejecutar el pipeline desde l√≠nea de comandos

Project Organization
------------

    ‚îú‚îÄ‚îÄ LICENSE
    ‚îú‚îÄ‚îÄ Makefile           <- Makefile with commands like `make data` or `make train`
    ‚îú‚îÄ‚îÄ README.md          <- The top-level README for developers using this project.
    ‚îú‚îÄ‚îÄ data
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ external       <- Data from third party sources.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim        <- Intermediate data that has been transformed.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed      <- The final, canonical data sets for modeling.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ raw            <- The original, immutable data dump.
    ‚îÇ
    ‚îú‚îÄ‚îÄ docs               <- A default Sphinx project; see sphinx-doc.org for details
    ‚îÇ
    ‚îú‚îÄ‚îÄ models             <- Trained and serialized models, model predictions, or model summaries
    ‚îÇ
    ‚îú‚îÄ‚îÄ notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),
    ‚îÇ                         the creator's initials, and a short `-` delimited description, e.g.
    ‚îÇ                         `1.0-jqp-initial-data-exploration`.
    ‚îÇ
    ‚îú‚îÄ‚îÄ references         <- Data dictionaries, manuals, and all other explanatory materials.
    ‚îÇ
    ‚îú‚îÄ‚îÄ reports            <- Generated analysis as HTML, PDF, LaTeX, etc.
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ figures        <- Generated graphics and figures to be used in reporting
    ‚îÇ
    ‚îú‚îÄ‚îÄ requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.
    ‚îÇ                         generated with `pip freeze > requirements.txt`
    ‚îÇ
    ‚îú‚îÄ‚îÄ setup.py           <- makes project pip installable (pip install -e .) so src can be imported
    ‚îú‚îÄ‚îÄ src                <- Source code for use in this project.
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py    <- Makes src a Python module
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data           <- Scripts to download or generate data
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ make_dataset.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ features       <- Scripts to turn raw data into features for modeling
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ build_features.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models         <- Scripts to train models and then use trained models to make
    ‚îÇ   ‚îÇ   ‚îÇ                 predictions
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ predict_model.py
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ train_model.py
    ‚îÇ   ‚îÇ
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ visualization  <- Scripts to create exploratory and results oriented visualizations
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ visualize.py
    ‚îÇ
    ‚îî‚îÄ‚îÄ tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io


--------

<p><small>Project based on the <a target="_blank" href="https://drivendata.github.io/cookiecutter-data-science/">cookiecutter data science project template</a>. #cookiecutterdatascience</small></p>
